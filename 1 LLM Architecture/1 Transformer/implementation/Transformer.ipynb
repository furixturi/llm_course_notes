{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer in Formula and Code\n",
    "\n",
    "Based on [Dive Into Deep Learning (D2L)](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "### Basics\n",
    "Denote a database of $m$ tuples of _keys_ and _values_ $D\\stackrel{def}{=}\\{(k_{1}, v_{1}), ..., (k_{m}, v_{m})\\}$, also denote a _query_ by $q$. Then we can define the _Attention_ over D as\n",
    "\n",
    "$$ Attention(q, D)\\stackrel{def}{=}\\sum_{i=1}^m \\alpha(q, k_{i})v_{i} $$ \n",
    "\n",
    "where $\\alpha(q, k_{i})$ are scalar attention weights. \n",
    "\n",
    "This operation pays more attention to terms where the weight is larger, hence the name _attention_.\n",
    "\n",
    "### Requirements\n",
    "To train a model using this function smoothly and stably, we want to ensure a number of requirements to the attention weights:\n",
    "- The weights $\\alpha(q, k_{i})$ are nonnegative.\n",
    "- The weights $\\alpha(q, k_{i})$ form a convex combination, i.e., $\\sum_{i}\\alpha(q,k_{i})=1$ and $\\alpha(q, k_{i})\\ge0$\n",
    "- Exactly one of the weights is 1 and all others are 0\n",
    "\n",
    "#### Sum to 1\n",
    "To ensure the weights sum up to 1, we can normalize them:\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = \\frac{\\alpha(q, k_{i})}{\\sum_{j}\\alpha(q,k_{j})} $$\n",
    "\n",
    "#### Non-negative\n",
    "To also ensure that the weights are non-negative, we can use exponentiation:\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = \\frac{exp(\\alpha(q, k_{i}))}{\\sum_{j}exp(\\alpha(q, k_{j}))} $$\n",
    "\n",
    "Now it is differentiable and its gradient never vanishes, all of which are desirable properties in a model.\n",
    "\n",
    "This is the $softmax$ operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product Attention Weights\n",
    "\n",
    "#### Gaussian kernel attention to dot product attention\n",
    "We can derive from Gaussain kernel attention to simple dot product attention naturally by expanding the formula then consider it after Batch and Layer Normalization. (Here queries $q$ and keys $k$ are column vectors, to take their dot product we transpose $q$.) \n",
    "\n",
    "$$ \\alpha(q, k_{i}) = q^\\top k_{i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The Gaussian kernel formula\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = -\\frac{1}{2} \\lVert q-k_{i} \\rVert^2 $$\n",
    "\n",
    "Can be expanded as\n",
    "\n",
    "$$ = q^\\top{k_{i}} - \\frac{1}{2}\\lVert k_{i} \\rVert^2 - \\frac{1}{2}\\lVert q \\rVert^2 $$\n",
    "\n",
    "For the last term $ - \\frac{1}{2}\\lVert q \\rVert^2 $, since $q$ is the same for all $(q, k_{i})$ pairs, it will disappear after normalization.\n",
    "\n",
    "For the term $- \\frac{1}{2}\\lVert k_{i} \\rVert^2$, since both batch and layer normalization lead to activations with well-bounded and often constant norms $\\lVert k_{i} \\rVert$, after normalization we can drop it without major change to the outcome as well.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled dot product attention weights\n",
    "\n",
    "Assume that all the elements of the query $ q \\in \\mathbb{R}^d $ and the key $ k \\in \\mathbb{R}^d $ are independent and identically drawn random variables with zero mean and unit variance. Their dot product will have zero mean and a variance of $d$. \n",
    "\n",
    "To keep the variance of the dot product to 1 regardless of the vector length $d$, we need to scale the dot product attention weights:\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = q^\\top k_{i} / \\sqrt{d} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we still need to normalize the weights $\\alpha$ to be non-negative and sum to one, so we apply the softmax and arrive at the most commonly used mechanism for attention weights - the scaled dot product attention weights.\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = softmax(\\alpha(q, k_{i})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\frac{exp(q^{\\top}k_{i}/\\sqrt{d})}{\\sum_{j=1}exp(q^{\\top}k_{j}/\\sqrt{d})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Matrix Multplication\n",
    "\n",
    "In practice, we use minibatches for computing efficiency, thus calculating the dot product attention for multiple queries at once. Say we compute attention for $n$ queries on $m$ key-value pairs, each query and key has the same dimension $d$, each value has dimension $v$. So we have three matrices: \n",
    "- $Q \\in \\mathbb{R}^{n \\times d}$, $n$ rows of $d$-dimensional query vectors\n",
    "- $K \\in \\mathbb{R}^{m \\times d}$, $m$ rows of $d$-dimensional key vectors\n",
    "- $V \\in \\mathbb{R}^{m \\times v}$, $m$ rows of $v$-dimensional value vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the scaled dot product attention of matrices $Q$, $K$, $V$ can be written as\n",
    "\n",
    "$$ softmax(\\frac{QK^\\top}{\\sqrt{d}})V \\in \\mathbb{R}^{n \\times v}$$\n",
    "\n",
    "The dimension of the calculated attention value matrix is $n \\times v$,  namely, one value vector row for each query row. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything, import the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a convenient softmax function optionally taking in a mask, so we can handle input sequences with different lengths (when they end up in the same minibatch, shorter sequences are typically padded with dummy tokens, to which we don't want our model to pay attention). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens = None):\n",
    "    \"\"\"\n",
    "    Perform softmax while optionally applying a sequence mask on the input X's last axis.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    X : 3D tensor (num_of_batches, batch_size, max_seq_len)\n",
    "        The input tensor with dimensions \n",
    "    valid_lens : 1D or 2D tensor\n",
    "        The valid length(s) of the sequence. Using a 1D tensor assumes the valid sequence length is the same in each batch. Using a 2D tensor allows the function to work with different sequence lengths in the same batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # helper function to apply mask to a sequnce\n",
    "    def _sequence_mask(X, valid_len, mask_value=0):\n",
    "        \"\"\"\n",
    "        Create a mask according to valid_len and return the a new tensor of X being applied the mask with mask_value\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        X : 2D tesnsor of dimension (num_sequences, max_seq_len)\n",
    "        valid_len : 1D tensor of dimension (1, num_sequences)\n",
    "        \"\"\"\n",
    "        maxlen = X.size(1) \n",
    "        seq_ids = torch.arange(maxlen, dtype=torch.float32, device=X.device)\n",
    "        mask = seq_ids[None, :] < valid_len[:, None]        \n",
    "        X[~mask] = mask_value\n",
    "        return X\n",
    "        \n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape # (num_of_batches x batch_size x padded_sequence_len)\n",
    "        if valid_lens.dim() == 1:\n",
    "            # if valid_lens is a 1D tensor, we want to make it 2D to work with our helper function by repeating it batch_size times \n",
    "            ## Intuition\n",
    "            ## Suppose X has 2 batches each containing 3 sequences and the max length of all sequences is 4, i.e., X.shape is (2 x 3 x 4) \n",
    "            ## Suppose valid_lens is a 1D tensor [2, 4] denoting that the first batch has sequence max_len 2, second batch 4\n",
    "            ## To convert it to the 2D tensor, since the batch size is 3 (shape[1]), we want our valid_lens tensor to be [[2, 2, 2], [4, 4, 4]]\n",
    "            # which is practically repeat each element of the original 1D tensor 3(shape[1]) times\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at this implementation. \n",
    "\n",
    "First the helper function `_sequence_mask(X, valid_len, mask_value=0)`:\n",
    "1. The input arguments:\n",
    "   1.  `X` is not directly the outer function's input argument X (which is a 3D tensor X of dimension (num_batches, batch_size, max_seq_len)), but its reshaped version of a 2D tensor, with dimensions `(num_sequences (aka num_batches * batch_size), max_seq_len)`.\n",
    "   2.  `valid_len` is not directly the outer function's input argument `valid_lens`, either. While the outer function accepts either a 1D or a 2D tensor, the inner function accepts only its sanitized and by necessity reshaped form, which is a 1D tensor of each sequence's valid length across all sequences in the input. Thus its dimension is (1, num_sequences)\n",
    "2. How the boolean mask is created:\n",
    "   1.  Get the max sequence length, which is the 2nd dimension of input `X`\n",
    "   2.  Use `torch.arange` and the max sequence length as its upper bound to create a vector of all position indices in a max length sequence, `seq_ids`\n",
    "   3.  Three steps to create the boolean mask for each sequence's each position\n",
    "      1.  expand the `seq_ids` 1D tensor to a 2D tensor by prepending a dimension, so that it's of dimension `(1, max_seq_len)`\n",
    "      2. expand the `valid_len` 1D tensor to a 2D tensor by appending a dimension, so that it's of dimension `(batch_size * num_batches, 1)`\n",
    "      3. create the boolean mask by element-wise comparing if a `seq_id` is smaller than the corresponding position's `valid_len`, which renders `True` if that position is within that sequence's valid length, `False` otherwise\n",
    "         1. Here we see a clever use of broadcasting: element-wise comparing two tensors of different dimensions will cause them to be both broadcasted. So both `seq_ids` and `valid_len` are broadcasted to be of dimension `(batch_size * num_batches, max_seq_len)`, essenially giving us a mask of the same dimension as this function's input X `(num_sequences, max_seq_len)`.\n",
    "3. How the boolean mask is applied\n",
    "   1. The boolean mask we created in step 2 has `True` in all positions that are within the valid length. Taking its complement `~mask` flips the `True` to `False`, now all the invalid positions will be `True` and we can use it to filter the invalid positions in `X`\n",
    "   2. `X[~mask]` selects all the invalid positions in X. Now we apply the mask value to these positions: `X[~mask] = mask_value`\n",
    "After all these, we return the masked X matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `DotProductAttention` class as a subclass of [PyTorch's `nn.Module`](https://pytorch.org/docs/stable/notes/modules.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
