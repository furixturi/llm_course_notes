{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Details and Implementation\n",
    "\n",
    "A theory to practice study of the Transformer model, based on relevant parts from the textbook [Dive Into Deep Learning (D2L)](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html). \n",
    "\n",
    "Note that the implementation is not for production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote a database of $m$ tuples of _keys_ and _values_ $D\\stackrel{def}{=}\\{(k_{1}, v_{1}), ..., (k_{m}, v_{m})\\}$, also denote a _query_ by $q$. Then we can define the _Attention_ over D as\n",
    "\n",
    "$$ Attention(q, D)\\stackrel{def}{=}\\sum_{i=1}^m \\alpha(q, k_{i})v_{i} $$ \n",
    "\n",
    "where $\\alpha(q, k_{i})$ are scalar attention weights. \n",
    "\n",
    "This operation pays more attention to terms where the weight is larger, hence the name _attention_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model using this function smoothly and stably, we want to ensure a number of requirements to the attention weights:\n",
    "- The weights $\\alpha(q, k_{i})$ are nonnegative.\n",
    "- The weights $\\alpha(q, k_{i})$ form a convex combination, i.e., $\\sum_{i}\\alpha(q,k_{i})=1$ and $\\alpha(q, k_{i})\\ge0$\n",
    "- Exactly one of the weights is 1 and all others are 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum to 1\n",
    "To ensure the weights sum up to 1, we can normalize them:\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = \\frac{\\alpha(q, k_{i})}{\\sum_{j}\\alpha(q,k_{j})} $$\n",
    "\n",
    "#### Non-negative\n",
    "To also ensure that the weights are non-negative, we can use exponentiation:\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = \\frac{exp(\\alpha(q, k_{i}))}{\\sum_{j}exp(\\alpha(q, k_{j}))} $$\n",
    "\n",
    "Now it is differentiable and its gradient never vanishes, all of which are desirable properties in a model.\n",
    "\n",
    "This is the $softmax$ operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian kernel attention to dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can derive from Gaussain kernel attention to simple dot product attention naturally by expanding the formula then consider it after Batch and Layer Normalization. (Here queries $q$ and keys $k$ are column vectors, to take their dot product we transpose $q$.) \n",
    "\n",
    "$$ \\alpha(q, k_{i}) = q^\\top k_{i} $$\n",
    "\n",
    "(The Gaussian kernel formula\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = -\\frac{1}{2} \\lVert q-k_{i} \\rVert^2 $$\n",
    "\n",
    "Can be expanded as\n",
    "\n",
    "$$ = q^\\top{k_{i}} - \\frac{1}{2}\\lVert k_{i} \\rVert^2 - \\frac{1}{2}\\lVert q \\rVert^2 $$\n",
    "\n",
    "For the last term $ - \\frac{1}{2}\\lVert q \\rVert^2 $, since $q$ is the same for all $(q, k_{i})$ pairs, it will disappear after normalization.\n",
    "\n",
    "For the term $- \\frac{1}{2}\\lVert k_{i} \\rVert^2$, since both batch and layer normalization lead to activations with well-bounded and often constant norms $\\lVert k_{i} \\rVert$, after normalization we can drop it without major change to the outcome as well.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled dot product attention weights\n",
    "\n",
    "Assume that all the elements of the query $ q \\in \\mathbb{R}^d $ and the key $ k \\in \\mathbb{R}^d $ are independent and identically drawn random variables with zero mean and unit variance. Their dot product will have zero mean and a variance of $d$. Here variance refers to how spread out the values are. When we compute the dot products of two vectors, higher dimensionality (vector length) will lead to higher variance.\n",
    "\n",
    "However, we want to keep the variance of the dot product to 1 regardless of the vector length $d$. The reason behind it are:\n",
    "1. Stable training: keeping the variance around a constant (like 1) helps stablize the gradient updates during training, decrease the risk of exploding gradients which destabilize the training\n",
    "2. Stable model: a constant variance means that the dot products don't change drastically with different vector lengths, making the model's behavior more predictable and stable.\n",
    "\n",
    "To achieve a constant variance of 1 for the dot product, we scale the dot product attention by the square root of the dimension:\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = q^\\top k_{i} / \\sqrt{d} $$\n",
    "\n",
    "Finally, we still need to normalize the weights $\\alpha$ to be non-negative and sum to one, so we apply the softmax and arrive at the most commonly used mechanism for attention weights - the scaled dot product attention weights.\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = softmax(\\alpha(q, k_{i})) $$\n",
    "$$ = \\frac{exp(q^{\\top}k_{i}/\\sqrt{d})}{\\sum_{j=1}exp(q^{\\top}k_{j}/\\sqrt{d})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multplication\n",
    "\n",
    "Calculating attention weights one query at a time is very slow. As GPU is highly optimized for matrix multiplication, we stack the query, key-value pairs into matrices to take advantage of the compute efficiency, calculating the dot product attention for multiple queries at once. \n",
    "\n",
    "Say we compute attention for $n$ queries on $m$ key-value pairs, each query and key has the same dimension $d$, each value has dimension $v$. So we have three matrices: \n",
    "- $Q \\in \\mathbb{R}^{n \\times d}$, $n$ rows of $d$-dimensional query vectors\n",
    "- $K \\in \\mathbb{R}^{m \\times d}$, $m$ rows of $d$-dimensional key vectors\n",
    "- $V \\in \\mathbb{R}^{m \\times v}$, $m$ rows of $v$-dimensional value vectors\n",
    "\n",
    "So the scaled dot product attention of matrices $Q$, $K$, $V$ can be written as\n",
    "\n",
    "$$ softmax(\\frac{QK^\\top}{\\sqrt{d}})V \\in \\mathbb{R}^{n \\times v}$$\n",
    "\n",
    "The dimension of the calculated attention value matrix is $n \\times v$,  namely, one value vector row for each query row. \n",
    "\n",
    "In practice, as each GPU has a limited memory, we normally can't calculate all queries at once, either. We will divide the query, key, value matricis into mini-batches. More about this will be elaborated later in the \"Batch Matrix Multiplication\" section of the implementation part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at a sample implementation of dot product attention in PyTorch. To understand the implementation details demanded by the training practicalities, we'll first look into a function `masked_softmax()`, which applies masked `softmax` to the calculated attention value, then look at the `DotProductAttention` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything, import the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The masked_softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Define a convenient softmax function which can optionally mask each sequence at the positions that go beyond its valid length. This allows us to handle input sequences with different lengths (when they end up in the same batch, shorter sequences are typically padded with dummy tokens, to which we don't want our model to pay attention). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens = None):\n",
    "    \"\"\"\n",
    "    Perform softmax while optionally applying a sequence mask on the input X's last axis.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    X : 3D tensor (num_batches, batch_size, max_seq_len)\n",
    "        The input batches of sequences\n",
    "    valid_lens : 1D or 2D tensor\n",
    "        The valid length of each sequence in the input batches. Using a 1D tensor assumes the valid sequence length is the same in each batch. Using a 2D tensor allows the function to work with different sequence lengths in the same batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # helper function to apply mask to a sequnce\n",
    "    def _sequence_mask(X, valid_len, mask_value=0):\n",
    "        \"\"\"\n",
    "        Returns a masked instance of the input sequences X. The mask_value will be applied to each sequence's positions which are not within that sequence's valid length as denoted in valid_len.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        X : 2D tesnsor of dimension (num_sequences, max_seq_len)\n",
    "            a matrix including all input sequences as rows\n",
    "        valid_len : 1D tensor of dimension (1, num_sequences)\n",
    "            the valid length of each input sequence\n",
    "        \"\"\"\n",
    "        max_seq_len = X.size(1) \n",
    "        mask = torch.arange(max_seq_len, dtype=torch.float32, \\\n",
    "            device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = mask_value\n",
    "        return X\n",
    "        \n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            # if valid_lens is a 1D tensor, we want to make it 2D to work with our helper function by repeating it batch_size times \n",
    "            ## Intuition\n",
    "            ## Suppose X has 2 batches each containing 3 sequences and the max length of all sequences is 4, i.e., X.shape is (2 x 3 x 4) \n",
    "            ## Suppose valid_lens is a 1D tensor [2, 4] denoting that the first batch has sequence max_len 2, second batch 4\n",
    "            ## To convert it to the 2D tensor, since the batch size is 3 (shape[1]), we want our valid_lens tensor to be [[2, 2, 2], [4, 4, 4]]\n",
    "            # which is practically repeat each element of the original 1D tensor 3(shape[1]) times\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            # otherwise, flatten the 2D tensor to 1D\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        \n",
    "        # Apply mask to replace all invalid positions (positions that exceed the valid length of that sequence) in all input sequences with a very small number -1e6\n",
    "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, mask_value = -1e6)\n",
    "        \n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at this implementation. \n",
    "\n",
    "First the helper function `_sequence_mask(X, valid_len, mask_value=0)`:\n",
    "1. The input arguments is preprocessed:\n",
    "   1.  `X` is not directly the outer function's input argument X (which is a 3D tensor X of dimension (num_batches, batch_size, max_seq_len)), but its reshaped version of a 2D tensor, with dimensions `(num_sequences (aka num_batches * batch_size), max_seq_len)`.\n",
    "   2.  `valid_len` is not directly the outer function's input argument `valid_lens`, either. While the outer function accepts either a 1D or a 2D tensor, the inner function accepts only its sanitized and by necessity reshaped form, which is a 1D tensor of each sequence's valid length across all sequences in the input. Thus its dimension is (1, num_sequences)\n",
    "2. How the boolean mask is created:\n",
    "   1.  Get the max sequence length `maxlen`, which is the 2nd dimension of input `X`\n",
    "   2.  Use `torch.arange` and the max sequence length as its upper bound to create a vector of all position ids in a longest sequence\n",
    "   3.  Directly in the same line of code, there were three steps to create the boolean mask for each sequence's each position\n",
    "      1.  expand the sequence ids 1D tensor to a 2D tensor by prepending a dimension at the front `[None, :]`, so that it's of dimension `(1, max_seq_len)`\n",
    "      2. expand the `valid_len` 1D tensor to a 2D tensor by appending a dimension at the back `[:, None]`, so that it's of dimension `(batch_size * num_batches, 1)`\n",
    "      3. create the boolean mask by element-wise comparing if a seq id is smaller than the corresponding position's `valid_len`, which renders `True` if the position is within that sequence's valid length, `False` otherwise\n",
    "         1. Here we see a clever use of broadcasting: element-wise comparing two tensors of different dimensions will cause them to be both broadcasted. So both seqence ids tensor and `valid_len` are broadcasted to be of dimension `(batch_size * num_batches, max_seq_len)`, essenially giving us a mask of the same dimension as this function's input X `(num_sequences, max_seq_len)`.\n",
    "3. How the boolean mask is applied\n",
    "   1. The boolean mask we created in step 2 has `True` in all positions that are within the valid length. Taking its complement `~mask` flips the `True` to `False`, now all the invalid positions will be `True` and we can use it to filter the invalid positions in `X`\n",
    "   2. `X[~mask]` selects all the invalid positions in X. Now we apply the mask value to these positions: `X[~mask] = mask_value`\n",
    "After all these, we return the masked X matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's see what was happening outside of the helper function, especially how the valid_lens tensor, if given, is normalized to a 1D tensor denoting the valid length of each sequence in all input sequence batches.\n",
    "1. If no valid_lens is given, simply do a softmax to the input matrix and return it\n",
    "2. If a valid_lens tensor is given, we need to check if it's 1D or 2D `shape = X.shape` Per our function definition, this shape should be `(num_batches, batch_size, max_seq_len)`\n",
    "   1. If it's a 1D tensor, it means each batch has a consistent valid length and it is denoted as each item in this 1D tensor. However, our helper function expects a 1D tensor denoting the valid length of each sequence, not each batch. To satisfy this, we can just copy each item of the input 1D tensor `batch_size` (the second dimension of the shape of X) times with `torch.repeat_interleave(valid_lens, shape[1])`\n",
    "   2. Else, it would be a 2D tensor, denoting the valid length of each sequence inside each batch. To get the 1D valid_lens tensor we need from this 2D tensor, we simply need to flatten it with `valid_lens.reshape(-1)` \n",
    "Thus the `valid_lens` is well prepared to be given to the helper function and get us a masked version of X. When calling the helper function, we specify a really small mask value `-1e6` to decrease the weight as much as possible, so that the model won't pay any attention to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: this implementation is directly from the textbook. From software engineering's perspective, the interface design is brittle without error handling, and the code lacks readability. I'll look into other implementations (e.g. [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#batches-and-masking)) later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `DotProductAttention` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the scaled dot product attention for baches. Say we compute attention for \n",
    "- $n$ queries of dimension $d$: $Q \\in \\mathbb{R}^{n \\times d}$, on\n",
    "- $m$ key-value pairs, where keys has the same dimension as queries: $K \\in \\mathbb{R}^{m \\times d}$, and\n",
    "- values in each of the $m$ pairs are all of length $v$: $V \\in \\mathbb{R}^{m \\times v}$\n",
    "\n",
    "$$ softmax(\\frac{QK^\\top}{\\sqrt{d}})V \\in \\mathbb{R}^{n \\times v} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `DotProductAttention` class as a subclass of [PyTorch's `nn.Module`](https://pytorch.org/docs/stable/notes/modules.html). `Dropout` is used for model regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot product attention\n",
    "    Parameters\n",
    "    ========\n",
    "    dropout: float between 0 and 1, optional\n",
    "        The dropout rate (probability of an element being set to zero) to apply to the attention weights before multiplying with the values. Default is 0.5\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the `forward` function. It's basically implementing the formula, with two twists:\n",
    "- apply dropout to the calculated weight scores before multiplying with values\n",
    "- use batch matrix multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a detour to batch matrix multiplication (BMM). \n",
    "\n",
    "For performance we divide the whole input matrix into $n$ minibatches. Matrix multiplication then will be performed elementwise on these sub matrices. Suppose we divided input matrices $A$ and $B$ into $n$ batches:\n",
    "- $A = [A_1, A_2, ..., A_n] \\in \\mathbb{R}^{n \\times a \\times b}$\n",
    "- $B = [B_1, B_2, ..., B_n] \\in \\mathbb{R}^{n \\times b \\times c}$\n",
    "\n",
    "Then the batch matrix multiplication (BMM) computes the elementwise product of them\n",
    "$$ BMM(A, B) = [A_1B_1, A_2B_2, ..., A_nB_n] \\in \\mathbb{R}^{n \\times a \\times c} $$\n",
    "\n",
    "As for our dot product attention, we also use batches, thus the dimensions of $Q$ `queries`, $K$ `keys`, and $V$ `values` are not 2D anymore, but 3D, with the first dimension being the `batch_size`, the other dimensions are as discussed earlier.\n",
    "- $Q$ `queries`: (batch_size, n, d)\n",
    "- $K$ `keys`: (batch_size, m, d)\n",
    "- $V$ `values`: (batch_size, m, v)\n",
    "\n",
    "To calculate the weight scores $QK^\\top$, we transpose the 2nd and 3rd dimensions of $K$ `keys.transpose(1, 2)`, then do batch matrix multiplication of $Q$ and transposed $K$ `torch.bmm(queries, keys.transpose(1,2))`.\n",
    "\n",
    "Later, after applying dropout to the calculated weights, BMM is also used to compute the attention values `torch.bmm(self.dropout(self.attention_weights), values)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Attention (Bahadanau Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mechanism in sequence-to-sequence model to improve the performance of neural machine translation systems. It addresses the limitation of encoding the entire input sequence into a fixed-size vector like in RNN, aiming to allow the decoder to focus on different parts of the input sequence during each step of the output generation.\n",
    "\n",
    "Its core attention weight calculation is a feed-forward neural network with a single hidden layer. $w_v$, $W_q$, $W_k$ are learnable parameters. It is additive with non-linearity (tanh activation), instead of simple dot product attention. \n",
    "\n",
    "$$ \\alpha(q,k) = w_v^\\top tanh(W_qq + W_kk) \\in{\\mathbb{R}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a sequence-to-sequence model, the context variable $c$ will be dynamically updated as a function of both the original text (encoder hidden states $h_t$) and the already generated text (decoder hidden states $s_{t'-1}$).\n",
    "\n",
    "$$ c_{t'} = \\sum_{t=1}^T \\alpha(s_{t'-1}, h_t)h_t $$\n",
    "\n",
    "We use decoder's previous hidden state $s_{t'-1}$ as the query, encoder's hidden states $h_t$ as both the key and the value, and calculate additive attention weight $\\alpha$. Later the model was modified to use the already generated tokens in decoder also as context, i.e., the attention sum does not stop at $T$, but rather $t'-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention and Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
