{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Details and Implementation\n",
    "\n",
    "A theory to practice study of the Transformer model, based on relevant parts from the textbook [Dive Into Deep Learning (D2L)](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html). \n",
    "\n",
    "Note that the implementation is not for production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote a database of $m$ tuples of _keys_ and _values_ $D\\stackrel{def}{=}\\{(k_{1}, v_{1}), ..., (k_{m}, v_{m})\\}$, also denote a _query_ by $q$. Then we can define the _Attention_ over D as\n",
    "\n",
    "$$ Attention(q, D)\\stackrel{def}{=}\\sum_{i=1}^m \\alpha(q, k_{i})v_{i} $$ \n",
    "\n",
    "where $\\alpha(q, k_{i})$ are scalar attention weights. \n",
    "\n",
    "This operation pays more attention to terms where the weight is larger, hence the name _attention_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a model using this function smoothly and stably, we want to ensure a number of requirements to the attention weights:\n",
    "- The weights $\\alpha(q, k_{i})$ are nonnegative.\n",
    "- The weights $\\alpha(q, k_{i})$ form a convex combination, i.e., $\\sum_{i}\\alpha(q,k_{i})=1$ and $\\alpha(q, k_{i})\\ge0$\n",
    "- Exactly one of the weights is 1 and all others are 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum to 1\n",
    "To ensure the weights sum up to 1, we can normalize them:\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = \\frac{\\alpha(q, k_{i})}{\\sum_{j}\\alpha(q,k_{j})} $$\n",
    "\n",
    "#### Non-negative\n",
    "To also ensure that the weights are non-negative, we can use exponentiation:\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = \\frac{exp(\\alpha(q, k_{i}))}{\\sum_{j}exp(\\alpha(q, k_{j}))} $$\n",
    "\n",
    "Now it is differentiable and its gradient never vanishes, all of which are desirable properties in a model.\n",
    "\n",
    "This is the $softmax$ operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian kernel attention to dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can derive from Gaussain kernel attention to simple dot product attention naturally by expanding the formula then consider it after Batch and Layer Normalization. (Here queries $q$ and keys $k$ are column vectors, to take their dot product we transpose $q$.) \n",
    "\n",
    "$$ \\alpha(q, k_{i}) = q^\\top k_{i} $$\n",
    "\n",
    "(The Gaussian kernel formula\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = -\\frac{1}{2} \\lVert q-k_{i} \\rVert^2 $$\n",
    "\n",
    "Can be expanded as\n",
    "\n",
    "$$ = q^\\top{k_{i}} - \\frac{1}{2}\\lVert k_{i} \\rVert^2 - \\frac{1}{2}\\lVert q \\rVert^2 $$\n",
    "\n",
    "For the last term $ - \\frac{1}{2}\\lVert q \\rVert^2 $, since $q$ is the same for all $(q, k_{i})$ pairs, it will disappear after normalization.\n",
    "\n",
    "For the term $- \\frac{1}{2}\\lVert k_{i} \\rVert^2$, since both batch and layer normalization lead to activations with well-bounded and often constant norms $\\lVert k_{i} \\rVert$, after normalization we can drop it without major change to the outcome as well.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled dot product attention weights\n",
    "\n",
    "Assume that all the elements of the query $ q \\in \\mathbb{R}^d $ and the key $ k \\in \\mathbb{R}^d $ are independent and identically drawn random variables with zero mean and unit variance. Their dot product will have zero mean and a variance of $d$. Here variance refers to how spread out the values are. When we compute the dot products of two vectors, higher dimensionality (vector length) will lead to higher variance.\n",
    "\n",
    "However, we want to keep the variance of the dot product to 1 regardless of the vector length $d$. The reason behind it are:\n",
    "1. Stable training: keeping the variance around a constant (like 1) helps stablize the gradient updates during training, decrease the risk of exploding gradients which destabilize the training\n",
    "2. Stable model: a constant variance means that the dot products don't change drastically with different vector lengths, making the model's behavior more predictable and stable.\n",
    "\n",
    "To achieve a constant variance of 1 for the dot product, we scale the dot product attention by the square root of the dimension:\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = q^\\top k_{i} / \\sqrt{d} $$\n",
    "\n",
    "Finally, we still need to normalize the weights $\\alpha$ to be non-negative and sum to one, so we apply the softmax and arrive at the most commonly used mechanism for attention weights - the scaled dot product attention weights.\n",
    "\n",
    "$$ \\alpha(q, k_{i}) = softmax(\\alpha(q, k_{i})) $$\n",
    "$$ = \\frac{exp(q^{\\top}k_{i}/\\sqrt{d})}{\\sum_{j=1}exp(q^{\\top}k_{j}/\\sqrt{d})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multplication\n",
    "\n",
    "Calculating attention weights one query at a time is very slow. As GPU is highly optimized for matrix multiplication, we stack the query, key-value pairs into matrices to take advantage of the compute efficiency, calculating the dot product attention for multiple queries at once. \n",
    "\n",
    "Say we compute attention for $n$ queries on $m$ key-value pairs, each query and key has the same dimension $d$, each value has dimension $v$. So we have three matrices: \n",
    "- $Q \\in \\mathbb{R}^{n \\times d}$, $n$ rows of $d$-dimensional query vectors\n",
    "- $K \\in \\mathbb{R}^{m \\times d}$, $m$ rows of $d$-dimensional key vectors\n",
    "- $V \\in \\mathbb{R}^{m \\times v}$, $m$ rows of $v$-dimensional value vectors\n",
    "\n",
    "So the scaled dot product attention of matrices $Q$, $K$, $V$ can be written as\n",
    "\n",
    "$$ softmax(\\frac{QK^\\top}{\\sqrt{d}})V \\in \\mathbb{R}^{n \\times v}$$\n",
    "\n",
    "The dimension of the calculated attention value matrix is $n \\times v$,  namely, one value vector row for each query row. \n",
    "\n",
    "In practice, as each GPU has a limited memory, we normally can't calculate all queries at once, either. We will divide the query, key, value matricis into mini-batches. More about this will be elaborated later in the \"Batch Matrix Multiplication\" section of the implementation part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at a sample implementation of dot product attention in PyTorch. To understand the implementation details demanded by the training practicalities, we'll first look into a function `masked_softmax()`, which applies masked `softmax` to the calculated attention value, then look at the `DotProductAttention` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything, import the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The masked_softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Define a convenient softmax function which can optionally mask each sequence at the positions that go beyond its valid length. This allows us to handle input sequences with different lengths (when they end up in the same batch, shorter sequences are typically padded with dummy tokens, to which we don't want our model to pay attention). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens = None):\n",
    "    \"\"\"\n",
    "    Perform softmax while optionally applying a sequence mask on the input X's last axis.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    X : 3D tensor (num_batches, batch_size, max_seq_len)\n",
    "        The input batches of sequences\n",
    "    valid_lens : 1D or 2D tensor\n",
    "        The valid length of each sequence in the input batches. Using a 1D tensor assumes the valid sequence length is the same in each batch. Using a 2D tensor allows the function to work with different sequence lengths in the same batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    # helper function to apply mask to a sequnce\n",
    "    def _sequence_mask(X, valid_len, mask_value=0):\n",
    "        \"\"\"\n",
    "        Returns a masked instance of the input sequences X. The mask_value will be applied to each sequence's positions which are not within that sequence's valid length as denoted in valid_len.\n",
    "        \n",
    "        Parameters\n",
    "        --------\n",
    "        X : 2D tesnsor of dimension (num_sequences, max_seq_len)\n",
    "            a matrix including all input sequences as rows\n",
    "        valid_len : 1D tensor of dimension (1, num_sequences)\n",
    "            the valid length of each input sequence\n",
    "        \"\"\"\n",
    "        max_seq_len = X.size(1) \n",
    "        mask = torch.arange(max_seq_len, dtype=torch.float32, \\\n",
    "            device=X.device)[None, :] < valid_len[:, None]\n",
    "        X[~mask] = mask_value\n",
    "        return X\n",
    "        \n",
    "    if valid_lens is None:\n",
    "        return nn.functional.softmax(X, dim=-1)\n",
    "    else:\n",
    "        shape = X.shape\n",
    "        if valid_lens.dim() == 1:\n",
    "            # if valid_lens is a 1D tensor, we want to make it 2D to work with our helper function by repeating each item of it batch_size times \n",
    "            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "        else:\n",
    "            # otherwise, flatten the 2D tensor to 1D\n",
    "            valid_lens = valid_lens.reshape(-1)\n",
    "        \n",
    "        # Apply mask to replace all invalid positions (positions that exceed the valid length of that sequence) in all input sequences with a very small number -1e6\n",
    "        X = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, mask_value = -1e6)\n",
    "        \n",
    "        return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at this implementation. \n",
    "\n",
    "First the helper function `_sequence_mask(X, valid_len, mask_value=0)`:\n",
    "1. The input arguments is preprocessed:\n",
    "   1.  `X` is not directly the outer function's input argument X (which is a 3D tensor X of dimension (num_batches, batch_size, max_seq_len)), but its reshaped version of a 2D tensor, with dimensions `(num_sequences (aka num_batches * batch_size), max_seq_len)`.\n",
    "   2.  `valid_len` is not directly the outer function's input argument `valid_lens`, either. While the outer function accepts either a 1D or a 2D tensor, the inner function accepts only its sanitized and by necessity reshaped form, which is a 1D tensor of each sequence's valid length across all sequences in the input. Thus its dimension is (1, num_sequences)\n",
    "2. How the boolean mask is created:\n",
    "   1.  Get the max sequence length `maxlen`, which is the 2nd dimension of input `X`\n",
    "   2.  Use `torch.arange` and the max sequence length as its upper bound to create a vector of all position ids in a longest sequence\n",
    "   3.  Directly in the same line of code, there were three steps to create the boolean mask for each sequence's each position\n",
    "      1.  expand the sequence ids 1D tensor to a 2D tensor by prepending a dimension at the front `[None, :]`, so that it's of dimension `(1, max_seq_len)`\n",
    "      2. expand the `valid_len` 1D tensor to a 2D tensor by appending a dimension at the back `[:, None]`, so that it's of dimension `(batch_size * num_batches, 1)`\n",
    "      3. create the boolean mask by element-wise comparing if a seq id is smaller than the corresponding position's `valid_len`, which renders `True` if the position is within that sequence's valid length, `False` otherwise\n",
    "         1. Here we see a clever use of broadcasting: element-wise comparing two tensors of different dimensions will cause them to be both broadcasted. So both seqence ids tensor and `valid_len` are broadcasted to be of dimension `(batch_size * num_batches, max_seq_len)`, essenially giving us a mask of the same dimension as this function's input X `(num_sequences, max_seq_len)`.\n",
    "3. How the boolean mask is applied\n",
    "   1. The boolean mask we created in step 2 has `True` in all positions that are within the valid length. Taking its complement `~mask` flips the `True` to `False`, now all the invalid positions will be `True` and we can use it to filter the invalid positions in `X`\n",
    "   2. `X[~mask]` selects all the invalid positions in X. Now we apply the mask value to these positions: `X[~mask] = mask_value`\n",
    "After all these, we return the masked X matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's see what was happening outside of the helper function, especially how the valid_lens tensor, if given, is normalized to a 1D tensor denoting the valid length of each sequence in all input sequence batches.\n",
    "1. If no valid_lens is given, simply do a softmax to the input matrix and return it\n",
    "2. If a valid_lens tensor is given, we need to check if it's 1D or 2D `shape = X.shape` Per our function definition, this shape should be `(num_batches, batch_size, max_seq_len)`\n",
    "   1. If it's a 1D tensor, it means each batch has a consistent valid length and it is denoted as each item in this 1D tensor. However, our helper function expects a 1D tensor denoting the valid length of each sequence, not each batch. To satisfy this, we can just copy each item of the input 1D tensor `batch_size` (the second dimension of the shape of X) times with `torch.repeat_interleave(valid_lens, shape[1])`\n",
    "      1. E.g. Suppose X has 2 batches each containing 3 sequences and the max length of all sequences is 4, i.e., X.shape is (2 x 3 x 4). Suppose valid_lens is a 1D tensor [2, 4] denoting that the first batch has sequence max_len 2, second batch 4. To convert it to the 2D tensor, since the batch size is 3 (shape[1]), we want our valid_lens tensor to be [[2, 2, 2], [4, 4, 4]], which is practically repeat each element of the original 1D tensor 3(shape[1]) times\n",
    "   2. Else, it would be a 2D tensor, denoting the valid length of each sequence inside each batch. To get the 1D valid_lens tensor we need from this 2D tensor, we simply need to flatten it with `valid_lens.reshape(-1)` \n",
    "Thus the `valid_lens` is well prepared to be given to the helper function and get us a masked version of X. When calling the helper function, we specify a really small mask value `-1e6` to decrease the weight as much as possible, so that the model won't pay any attention to them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: this implementation is directly from the textbook. From software engineering's perspective, the interface design is brittle without error handling, and the code lacks readability. I'll look into other implementations (e.g. [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#batches-and-masking)) later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `DotProductAttention` Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the scaled dot product attention for baches. Say we compute attention for \n",
    "- $n$ queries of dimension $d$: $Q \\in \\mathbb{R}^{n \\times d}$, on\n",
    "- $m$ key-value pairs, where keys has the same dimension as queries: $K \\in \\mathbb{R}^{m \\times d}$, and\n",
    "- values in each of the $m$ pairs are all of length $v$: $V \\in \\mathbb{R}^{m \\times v}$\n",
    "\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^\\top}{\\sqrt{d}})V \\in \\mathbb{R}^{n \\times v} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `DotProductAttention` class as a subclass of [PyTorch's `nn.Module`](https://pytorch.org/docs/stable/notes/modules.html). `Dropout` is used for model regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot product attention\n",
    "    Parameters\n",
    "    ========\n",
    "    dropout: float between 0 and 1, optional\n",
    "        The dropout rate (probability of an element being set to zero) to apply to the attention weights before multiplying with the values. Default is 0.5\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, queries, keys, values, valid_lens=None):\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the `forward` function. It's basically implementing the formula, with two twists:\n",
    "- apply dropout to the calculated weight scores before multiplying with values\n",
    "- use batch matrix multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a detour to batch matrix multiplication (BMM). \n",
    "\n",
    "For performance we divide the whole input matrix into $n$ minibatches. Matrix multiplication then will be performed elementwise on these sub matrices. Suppose we divided input matrices $A$ and $B$ into $n$ batches:\n",
    "- $A = [A_1, A_2, ..., A_n] \\in \\mathbb{R}^{n \\times a \\times b}$\n",
    "- $B = [B_1, B_2, ..., B_n] \\in \\mathbb{R}^{n \\times b \\times c}$\n",
    "\n",
    "Then the batch matrix multiplication (BMM) computes the elementwise product of them\n",
    "$$ BMM(A, B) = [A_1B_1, A_2B_2, ..., A_nB_n] \\in \\mathbb{R}^{n \\times a \\times c} $$\n",
    "\n",
    "As for our dot product attention, we also use batches, thus the dimensions of $Q$ `queries`, $K$ `keys`, and $V$ `values` are not 2D anymore, but 3D, with the first dimension being the `batch_size`, the other dimensions are as discussed earlier.\n",
    "- $Q$ `queries`: (batch_size, n, d)\n",
    "- $K$ `keys`: (batch_size, m, d)\n",
    "- $V$ `values`: (batch_size, m, v)\n",
    "\n",
    "To calculate the weight scores $QK^\\top$, we transpose the 2nd and 3rd dimensions of $K$ `keys.transpose(1, 2)`, then do batch matrix multiplication of $Q$ and transposed $K$ `torch.bmm(queries, keys.transpose(1,2))`.\n",
    "\n",
    "Later, after applying dropout to the calculated weights, BMM is also used to compute the attention values `torch.bmm(self.dropout(self.attention_weights), values)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Attention (Bahadanau Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mechanism in sequence-to-sequence model to improve the performance of neural machine translation systems. It addresses the limitation of encoding the entire input sequence into a fixed-size vector like in RNN, aiming to allow the decoder to focus on different parts of the input sequence during each step of the output generation.\n",
    "\n",
    "Its core attention weight calculation is a feed-forward neural network with a single hidden layer. $w_v$, $W_q$, $W_k$ are learnable parameters. It is additive with non-linearity (tanh activation), instead of simple dot product attention. \n",
    "\n",
    "$$ \\alpha(q,k) = w_v^\\top tanh(W_qq + W_kk) \\in{\\mathbb{R}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a sequence-to-sequence model, the context variable $c$ will be dynamically updated as a function of both the original text (encoder hidden states $h_t$) and the already generated text (decoder hidden states $s_{t'-1}$).\n",
    "\n",
    "$$ c_{t'} = \\sum_{t=1}^T \\alpha(s_{t'-1}, h_t)h_t $$\n",
    "\n",
    "We use decoder's previous hidden state $s_{t'-1}$ as the query, encoder's hidden states $h_t$ as both the key and the value, and calculate additive attention weight $\\alpha$. Later the model was modified to use the already generated tokens in decoder also as context, i.e., the attention sum does not stop at $T$, but rather $t'-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pay attention to different representation subspaces, aka to learn different aspects of the input data (e.g., syntactic, semantic, dependencies of various ranges) , we run attention mechanism multiple times in parallel. Each parallel run is called an \"attention head\". \n",
    "\n",
    "Each of these $h$ heads independently learns queries, keys, and values' linear pojections, which are then fed into attention pooling in parallel. These $h$ attention pooling outputs are then concatenated and transformed with another learned linear projection and produce the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "Mathematical formalizatioin of multi-head attention. Given a query of length $d_q$, $q \\in \\mathbb{R}^{d_q}$, a key of length $d_k$, $k \\in \\mathbb{R}^{d_k}$, and its corresponding value of length $d_v$, $v \\in \\mathbb{R}^{d_v}$, each attention head $head_i$ is computed as:\n",
    "\n",
    "$$ head_i = Attention(W_i^qq, W_i^kk, W_i^vv) \\in \\mathbb{R}^{p_v} $$\n",
    "\n",
    "where $W_i^q \\in \\mathbb{R}^{p_q \\times d_q}$, $W_i^{k} \\in \\mathbb{R}^{p_k \\times d_k}$, $W_i^{v} \\in \\mathbb{R}^{p_v \\times d_v}$ are learnable parameters. $p_q$, $p_k$, $p_v$ are number of rows (or number of hidden units) of each weight matrix, initially equal to the corresponding input's word embedding dimension divided by the number of heads (so each head can focus on different vector subspace). In the original paper the embedding dimension is 512 and the number of heads is 8, so all of $p_q$, $p_k$, and $p_v$ equal to $ 512 / 8 = 64$, which is dimension per head, named $d_{model}$ (`d_model`).\n",
    "\n",
    "The $Attention$ function can be for example additive attention or scaled dot product attention.\n",
    "\n",
    "Then all the $h$ heads' outputs are concatenated so we have the same dimension as the input embedding vector space, 512, for the output. The concatenated output then go through a linear transformation via learnable weights $W^o$, so each head's learning is synthesized and we get the final output representation:\n",
    "$$ W^o \\begin{bmatrix} head_1 \\\\ \\vdots \\\\ head_h\\end{bmatrix} \\in \\mathbb{R}^{p_o} $$\n",
    "\n",
    "Thus the multihead attention:\n",
    "$$ MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^o $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following implementation, $p_o$ is represented by the parameter `num_hiddens`. For input we use batches. In each batch there are `batch_size` number of sentences (sequences), each sequence is composed of `query_size` number of queries, or `key_size`/`value_size` number of key-value pairs.\n",
    "\n",
    "We create a `MultiHeadAttention` module and use the `DotProductAttention` module defined earlier for each head.  The `valid_lens` is also used as `DotProductAttention` uses the `masked_softmax(X, valid_lens)` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transpose_qkv()` function splits the `num_hiddens` into two dimensions: `num_heads` and `d_model`, rearranges the order of the dimensions, then multiplies `batch_size` and `num_heads` and put their product as the first dimension. Deep learning frameworks like PyTorch treats the first dimension of the input matrix as the batch dimension and will process elements in this dimension in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs):\n",
    "        super().__init()__\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q = nn.LazyLinear(num_hiddens, bias=bias) # num_hiddens will later be split into multiheads in transpose_qkv()\n",
    "        self.W_k = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_v = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        self.W_o = nn.LazyLinear(num_hiddens, bias=bias)\n",
    "        \n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        queries = self.transpose_qkv(self.W_q(queries))\n",
    "        keys = self.transpose_qkv(self.W_k(keys))\n",
    "        values = self.transpose_qkv(self.W_v(values))\n",
    "        \n",
    "        # if valid_lens is specified, copy it num_heads times to use for all heads\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "            \n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        output_concat = self.transpose_output(output)\n",
    "        return self.W_o(output_concat)\n",
    "    \n",
    "    def transpose_qkv(self, X):\n",
    "        \"\"\"split the input embedding vector space for multi-heads parallel computation\"\"\"\n",
    "        batch_size, num_qkv, num_hiddens = X.shape\n",
    "        # split the num_hiddens dimension into two dimensions, one is num_heads, the other is num_hiddens/num_heads aka d_model (calculated automatically by providing the last argument as -1)\n",
    "        X = X.reshape(batch_size, num_qkv, self.num_heads, -1)\n",
    "        # then rearrange the dimensions to prepare for the next reshape\n",
    "        X = X.permute(0, 2, 1, 3) # (batch_size, num_heads, num_qkv, d_model)\n",
    "        # merge the first two dimensions, batch_size and num_heads, into one single dimension as the first dimension. This results in a tensor suitable for parallel computation across all heads\n",
    "        return X.reshape(-1, X.shape[2], X.shape[3]) # (batch_size * num_heads, num_qkv, d_model)\n",
    "    \n",
    "    def transpose_output(self, X):\n",
    "        \"\"\"concat each head's output\"\"\"\n",
    "        batch_size_all_heads, num_qkv, d_model = X.shape\n",
    "        # separate batch_size and num_heads from the first dimension\n",
    "        X = X.reshape(-1, self.num_heads, num_qkv, d_model) # (batch_size, num_heads, num_qkv, d_model)\n",
    "        X = X.permute(0, 2, 1, 3) # (batch_size, num_qkv, num_heads, d_model)\n",
    "        # merge the last two dimensions, num_heads and d_model, to get the original vector embedding dimension (64 * 8 = 512)\n",
    "        return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention and Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
